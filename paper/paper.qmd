---
title: "[Result, but poetic]: Does LLM Use Improve Data Science Education?"
subtitle: "Evidence from a Canadian Undergraduate Statistics Course"
author: 
  - Rohan Alexander
  - Luca Carnegie
thanks: "Code and data are available at: https://github.com/lcarnegie/llms-achievement."
date: today
date-format: long
abstract: "Blah Blah Blah Blah"
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
```


# Introduction

The rapid advancement of Large Language Models (LLMs) like ChatGPT has had massive effects in all areas of society, with some increases in productivity within some knowledge-based industries. Recent papers have demonstrated evidence of some productivity increases, in industries ranging from consulting to computer programming.  

On the lower end of productivity increases, Ben-Michael et al. (2024) conducted a randomized control trial with judges in Wisconsin to assess the impact of AI recommendations on bail hearing decisions. The study found that providing AI recommendations did not significantly improve judges' decision-making accuracy. 
On the other hand, Peng et al. (2023) investigated the significantly positive impact of GitHub Copilot, an LLM-powered coding assistant, on programmer productivity. They conducted an experiment with 95 professional programmers recruited through Upwork. The study found that programmers with access to Copilot completed a standardized programming task 55.8% faster than the control group. In particular, less experienced programmers appeared to benefit more from the tool. 

A study by Dell'Acqua et al. (2023) examined the impact of AI on management consulting tasks through a field experiment with 758 Boston Consulting Group consultants. They found that AI significantly increased performance across various business tasks, with speed increasing by over 25% and quality by more than 40%. However, the study revealed a "jagged technological frontier" where AI excelled in some tasks but struggled with others. Notably, AI usage appeared to level performance differences across ability levels, with lower-performing consultants benefiting the most, similar to that of Peng et al. 

We can see in the current literature that LLMs have had varying levels of impact across different knowledge-intensive industries, with more computer-intensive tasks like programming receiving the highest performance boosts. One area sorely lacking quantitative evidence of performance increases is within higher education, an area where scrutiny of the impact of LLMs and other AI tools has been common. Cahill and McCabe (2023) conducted a survey of undergraduate political science students, revealing widespread use of AI tools like ChatGPT. However, their study also uncovered a significant gap in students' perceived competence in using these tools effectively for academic work, though it remains to be seen if this gap is smaller in numerical-based fields like statistics and data science. 

Given the rapid integration of AI tools in educational and professional settings, there is an urgent need to understand how these technologies affect student learning and performance, particularly in fields like data science and statistics. This paper aims to investigate the relationship between LLM usage and academic performance, using evidence from an exit survey of a third-year undergraduate statistics course at the University of Toronto. By examining [how students interact with AI tools and how this usage correlates with their grades, we seek to understand the potential benefits and pitfalls of AI integration in data science education].

The remainder of this paper is structured as follows: @sec-data analyzes our survey data; @sec-model models our unstructured data to find fun and cool things; @sec-results lists the results; @sec-discussion discusses the implications of these findings for data science education and for future research and practice in this rapidly evolving field.



# Data {#sec-data}



# Model {#sec-model}

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results {#sec-results}

Our results are summarized in @tbl-modelresults.







# Discussion {#sec-discussion}

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In ... we implement a posterior predictive check. This shows...

In ... we compare the posterior with the prior. This shows... 


## Diagnostics

... is a trace plot. It shows... This suggests...

... is a Rhat plot. It shows... This suggests...



\newpage


# References


