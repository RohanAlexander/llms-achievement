---
title: "[Result, but poetic]: Does LLM Use Improve Data Science Education?"
subtitle: "Evidence from a Canadian Undergraduate Statistics Course"
author: 
  - Rohan Alexander
  - Luca Carnegie
thanks: "Code and data are available at: https://github.com/lcarnegie/llms-achievement."
date: today
date-format: long
abstract: "An abstract should eventually go here!!"
format: pdf
number-sections: true
bibliography: references.bib
nocite: | #references.bib entries at end, regardless of intext citation
  @* 
---

```{r setup, include=FALSE}
## Workspace Setup ##

library(tidyverse)
library(knitr)
library(kableExtra)
library(lintr)
library(tidyr)
library(tidytext)
library(textdata)
library(wordcloud)
library(reshape2)


# Import data
survey_data <- read_csv("../data/analysis_data/clean_STA302_postcourse_survey_w24.csv", show_col_types = FALSE)
```

# Introduction

The wide release of ChatGPT and other Large Language Models (LLMs) has rapidly transformed education at all levels and disciplines. Their quick adoption in academic settings has created excitement and apprehension among educators and students alike. LLMs offer unprecedented support for students in tasks ranging from writing assistance to support in complex problem-solving, with the potential to enhance academic performance and student outcomes. At the same time, their use raises important questions about academic integrity, the development of critical thinking skills (particular in undergraduates), as well as questions about their implications for effective learning overall. 

The potential for LLMs to positively affect students' academic performance in data science can be clearly inferred from already-demonstrated effects on adjacent professional fields. The tasks involved in a data science workflow can be generally broken down into two key competencies. The first is programming, which is done when cleaning, analyzing, visualizing data using languages like R or Python. The second is writing, which is primarily done when communicating results. There is experimental evidence from adjacent professional fields that LLMs can improve productivity in both of these competencies. 

@Peng_2023 show the potential impact of LLMs on students' computer programming skills, through their uncovering of the significantly positive impacts of GitHub Copilot (an LLM-powered programming assistant) on the productivity of computer programmers. In an experiment involving 95 freelance programmers, they found that that programmers with access to Copilot completed a standardized programming task 55.8% faster than the control group. Crucially, it was found that programmers with less experience saw the greatest improvements in productivity. Given that programming is a core component of data science practice, this suggests that these benefits could reasonably carry over to students in data science courses as well. 

@DellAcqua_2023, while primarily about management consulting tasks, provides evidence that LLMs can significantly improve writing productivity. In a field experiment involving Consultants recruited from the Boston Consulting Group (BCG), they found that the use of GPT-4 in the experimental group led to a 25% increase in delivery speed of business tasks (most involving some writing), as well as a 40% increase in human-rated performance on those tasks. Similar to computer programming, these productivity increases were most pronounced for those with below average performance, with their output increasing by 43%. Though not all tasks done using AI saw the same productivity improvements, the authors expressed particular optimism for LLMs' potential to generally expedite menial knowledge-work tasks. These included tasks such as generating new ideas and creating persuasive and informative writing - both examples directly apply to data science education through their analogues of coming up with project ideas and writing more engaging reports. As a whole, they show that LLMs could equally augment the quality of student writing and course projects in data science as well. 

Clearly, the implications of these case studies appear promising. However, literature from educators both adjacent to and distant from statistics and data science suggest much more varied perspectives on how LLMs can impact student performance.

@Valenzuela_2024 argue that LLMs lead to a loss of serendipity (which leads to less original work) and de-skilling (primarily with respect to programming ability), among other consequences. These particular outcomes could negatively affect students' effective learning of data science. On the other hand, @Ellis_Slade_2023 take a more optimistic perspective, taking the popular stance of comparing ChatGPT to previously controversial learning technologies like calculators. They argue that LLMs are just another technology that will impact Statistics and Data Science education like the calculator did. However, they take a broader perspective on the issue and more specific inquiry could be done in how effective LLMs are at improving student outcomes within data science education in particular. 

In a similar attitude, @Tu_2024 acknowledge that for students, LLMs can streamline many parts of a data science workflow - with that in mind, they suggest that data scientists in training should shift their self-perspectives from primarily being an analyst to primarily being a manager responsible for strategic oversight of the analysis. Crucially they emphasize that in both education and practice, LLMs and human intelligence should play complementary roles. 

The simultaneous caution and interest the literature expressed toward using LLMs in educational settings was further corroborated by empirical studies on K-12 and university students as well. 

At the high school level, @Lazar_2023 conducted a informal survey of secondary school teachers and students on their opinions of ChatGPT, they found that while LLMs could help spark creativity, provide academic support when teachers were unavailable, and model certain types of writing well, teachers were also cognizant of LLMs potential to limit students' learning in certain ways through overreliance. Beyond academic integrity concerns, teachers had similar concerns to @Valenzuela_2024 about de-skilling and an overall loss of agency in writing and critical thinking.

@Cahill_2024 surveyed undergraduate Political Science students on their attitudes toward and usage of AI tools. They found that the use of ChatGPT (among other machine-learning-powered software) was widespread. However, they also found that many students lacked the confidence in using AI for academic purposes - in particular, only 11% 'strongly agreed' that they know how to use AI to improve their writing. Like educators,  students have nuanced views on appropriate AI use. In particular, respondents found that using it to writing whole papers as inappropriate, while using it for basic tasks like general assistance, writing feedback and basic data visualization was perceived more appropriately. Interestingly, first-generation college students were found to be more likely to use AI in their work, particularly in writing papers and helping with assignments. Their findings suggest that LLMs and other AI tools could be an equalizer for disadvantaged students. Rhough only political science students were surveyed, statistics and data science students could reasonably have general attitudes that are similar. 

As we have seen, the existing inquiry by educators has explored the general qualitative usage and student perceptions on these tools. Though informative, a key question still remains: how can students' academic performance be affected, precisely, through allowing them to use LLMs in classwork? 

This paper aims to fill this gap by quantitatively investigating the relationship between students' grades and measures of student LLM usage and their attitudes toward LLMs in general, using evidence from a third-year undergraduate statistics course at the University of Toronto. Through examining how students interact with and perceive LLM tools and how these variables translate into student outcomes, the effects of LLM integration in data science education can be more precisely determined.

The remainder of this paper is structured as follows: @sec-data visualizes and analyzes survey data and coursework from students; @sec-model models the unstructured data to approximate a relationship between grades and usage/attitudes; @sec-results describes and analyzes the model's results; @sec-discussion discusses the implications of the findings for data science education and future research and practice in this rapidly evolving field.


# Data {#sec-data}

## Background and Collection

To investigate students' usage and attitudes toward LLM use and how they related to their academic performance, a dataset containing their usage/attitudes, coursework, and academic performance was constructed. Data was sampled from the cohort of students taking STA302 - Methods of Data Analysis I, taught by one of the investigators in the Winter 2024 semester at the University of Toronto. By virtue of pre-requisites needed, this restricted the data collected to be only on upper-year undergraduate students. 

Data was collected from students through an optional end-of-course survey. Whether or not they consented to their data being used in this investigation, all respondents received a +1% increase in their final course grade for their participation. All consenting responses were then cross-referenced to their course grade, as well as the GitHub account they used to complete their course research papers. The responses were finally anonymized by removing any personal references to the students themselves. 

## Cleaning and Variables of Interest

All the data was cleaned using R [@citeR] and it's tidyverse [@tidyverse], janitor [@janitor] and stringr [@stringr], then tested for issues using the testthat package [@testthat]. This led to a final dataset containing 121 responses across 41 variables that corresponded to each survey question. 

```{r fig-boilerplate}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: (CHANGE) Boilerplate figure caption 
#| fig-align: center
# survey_data |>
#   ggplot(program_counts, aes(x = "", y = n, fill = program_of_study)) +
#   #put geom type here (put before compilation) +
#   labs(title = "Title!",
#        x = "X label",
#        y = "Y label") +
#   theme_minimal() +
#   scale_fill_discrete(name="Legend Title") +
#   theme(plot.title.position = "plot",
#         plot.title = element_text(hjust = 0.5)
#         )
```

```{r tbl-boilerplate}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: (CHANGE) Boilerplate table caption 
#| fig-align: center
#Put code below this line
```

## General Information

First, general information about the sample of students was derived first, through visualizing the basic data about them. This was done using the ggplot2 [@ggplot2] package.

```{r fig-response-time-hist}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Distribution of Survey Response Times
#| fig-align: center

# Trim values to histogram

plot_data <- survey_data |>
  filter(mins_complete < 1000)


plot_data |>
  ggplot(aes(x = mins_complete)) +
  geom_histogram(binwidth = 2, fill = "skyblue", color = "black") +
  labs(
    title = "Distribution of Survey Response Times",
    x = "Time (Mins)",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(hjust = 0.5)
  )
```
@fig-response-time-hist shows the distribution of response times to the end-of-course survey. Respondents who consented generally took between 0-40 mins to answer all of the questions, with most response times ranging between 2-10 minutes. There was one consenting respondent who took 4308 minutes, whose observation was dropped from the visualization. This suggests that students generally took a meaningful amount of time to engage with the survey.  


```{r fig-gpa-hist}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Distribution of Student GPA
#| fig-align: center

# Histogram of GPA

survey_data |>
  ggplot(aes(x = gpa)) +
  geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black") +
  labs(
    title = "Distribution of Student GPA",
    x = "GPA",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(hjust = 0.6)
  )
```

@fig-gpa-hist shows a wide distribution of student GPAs, with the majority clustering around a B (3.0/4.0) average. One factor possibly affecting the range is the course being a required course for programs in the Statistics, Mathematics and Computer Science departments. Though programs in these departments generally attract high-achieving students, some may still struggle. Another factor is that these GPAs were self-reported, which introduces some ambiguity - students may have provided either their cumulative GPA or their most recent term's GPA, or could have misreported it entirely.  Despite these two factors, the diversity in reported grades suggests the course attracted students across a broad spectrum of academic performance levels.  


Get inspired by Cahill and McCabe's paper for visualizations etc. 

```{r tbl-year-dist}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Distribution of Students by Year
#| fig-align: center

# Change PEY to 4th year
survey_data <- survey_data |>
  mutate(
    year = case_when(
      year == "PEY" ~ "4th",
      year == "5th" ~ "5th+",
      year == "6th" ~ "5th+",
      TRUE ~ year
    )
  )

year_count <- survey_data |>
  count(year) |>
  pivot_wider(names_from = year, values_from = n, values_fill = list(n = 0))

# Display the original wide format table
kable(year_count)
```

We can also see in from @tbl-year-dist that students from a range of years took the course. That said, the majority of students were in their 3rd or 4th year of study. This makes sense, given the course's prerequisite of general statistics, which is a two-semester sequence typically done in second year statistics. This also explains the lack of any first year students in the course as well. 

```{r fig-programs-pie}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Student Distribution by Program of Study
#| fig-align: center

# Pie chart of program of study

program_counts <- survey_data |>
  count(program_of_study)

ggplot(program_counts, aes(x = "", y = n, fill = program_of_study)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y") +
  labs(
    title = "Student Distribution by Program of Study",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  scale_fill_discrete(name = "Program") +
  theme(
    axis.text.x = element_blank(), # Remove x-axis text
    axis.title.x = element_blank(), # Remove x-axis title
    plot.title = element_text(hjust = 1.6)
  ) +
  geom_text(aes(label = n),
    position = position_stack(vjust = 0.5)
  )
```

Programs of study [did something; waiting on major/minor data], as we can see in @fig-programs-pie... 


## Student Attitudes 

```{r fig-ai-familiarity}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Student Familiarity with AI
#| fig-align: center

table_data <- survey_data |>
  select(ai_familiarity) |>
  mutate(
    ai_familiarity = case_when(
      ai_familiarity == "Limited to some liberal arts, such as history courses" ~ "Somewhat familiar",
      TRUE ~ ai_familiarity
    )
  ) |>
  count(ai_familiarity)

table_data |>
  ggplot(aes(x = "", y = n, fill = ai_familiarity)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y") +
  labs(
    title = "Student Familiarity with AI",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  scale_fill_discrete(name = "Program") +
  theme(
    axis.text.x = element_blank(), # Remove x-axis text
    axis.title.x = element_blank(), # Remove x-axis title
    plot.title = element_text(hjust = 1.35)
  ) +
  geom_text(aes(label = n),
    position = position_stack(vjust = 0.5)
  )
```

Just as @Cahill_2024 found, the vast majority of students taking the course were at least "somewhat familiar" with ChatGPT and other LLMs. This overrepresentation of familiarity also primarily driven by the fact that the course is primarily for students in Statistics, Computer Science, Economics, Mathematics and other quantitative disciplines, which could likely lead to a skewing of interest/affinity for computers. 


Students were then asked about their attitudes and self-perception on writing and coding.  

```{r fig-writing-multibar-1}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Student Self-Perceptions on Writing, Part 1
#| fig-align: center

# Create a multi-bar plot for writing perceptions grouped by response

# Select first three statements
writing_perceptions <- survey_data |>
  select(
    writing_is_easy_for_me,
    i_like_to_write,
    i_believe_it_is_important_to_be_a_good_writer
  )

writing_perceptions_long <- writing_perceptions |>
  pivot_longer(cols = everything(), names_to = "Statements", values_to = "Response") |>
  mutate(
    Statements = case_when(
      Statements == "writing_is_easy_for_me" ~ "\"Writing is easy for me\"",
      Statements == "i_like_to_write" ~ "\"I like to write\"",
      Statements == "i_believe_it_is_important_to_be_a_good_writer" ~ "\"I believe it is important\n to be a good writer\"",
      TRUE ~ Statements
    ),
    Response = case_when(
      Response == "This is a lot like me" ~ "A lot like me",
      Response == "This is very different to me" ~ "Very different from me",
      Response == "This somewhat describes me" ~ "Somewhat describes me",
    )
  ) |>
  filter(!is.na(Response))

# Taken from: https://stackoverflow.com/questions/52297978/decrease-overal-legend-size-elements-and-text
addSmallLegend <- function(myPlot, pointSize = 0.5, textSize = 3, spaceLegend = 0.1) {
  myPlot +
    guides(
      shape = guide_legend(override.aes = list(size = pointSize)),
      color = guide_legend(override.aes = list(size = pointSize))
    ) +
    theme(
      legend.title = element_text(size = textSize),
      legend.text = element_text(size = textSize),
      legend.key.size = unit(spaceLegend, "lines")
    )
}

# Make the plot
multi_bar <- writing_perceptions_long |>
  ggplot(aes(x = Response, fill = Statements)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(
    title = "Student Self-Perceptions on Writing",
    x = "Response",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom",
  )

multi_bar <- addSmallLegend(multi_bar, pointSize = 0.5, textSize = 10, spaceLegend = 0.8)

multi_bar

# TODO: Fix label, Fix Legend title, Fix legibility of Response
```

```{r fig-writing-multibar-2}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Student Self-Perceptions on Writing, Part 2
#| fig-align: center

# Create a multi-bar plot for writing perceptions grouped by response

# Select last three statements
writing_perceptions <- survey_data |>
  select(
    when_i_edit_it_is_easy_for_me_to_catch_my_mistakes,
    i_feel_confident_sharing_my_writing,
    i_am_confident_in_my_overall_writing_ability
  )

writing_perceptions_long <- writing_perceptions |>
  pivot_longer(cols = everything(), names_to = "Statements", values_to = "Response") |>
  mutate(
    Statements = case_when(
      Statements == "when_i_edit_it_is_easy_for_me_to_catch_my_mistakes" ~ "\"When I edit, it is easy to\n catch my own mistakes\"",
      Statements == "i_feel_confident_sharing_my_writing" ~ "\"I feel confident\n sharing my writing\"",
      Statements == "i_am_confident_in_my_overall_writing_ability" ~ "\"I am confident in\n my own writing ability\"",
      TRUE ~ Statements
    ),
    Response = case_when(
      Response == "This is a lot like me" ~ "A lot like me",
      Response == "This is very different to me" ~ "Very different from me",
      Response == "This somewhat describes me" ~ "Somewhat describes me",
    )
  ) |>
  filter(!is.na(Response))

# Taken from: https://stackoverflow.com/questions/52297978/decrease-overal-legend-size-elements-and-text
addSmallLegend <- function(myPlot, pointSize = 0.5, textSize = 3, spaceLegend = 0.1) {
  myPlot +
    guides(
      shape = guide_legend(override.aes = list(size = pointSize)),
      color = guide_legend(override.aes = list(size = pointSize))
    ) +
    theme(
      legend.title = element_text(size = textSize),
      legend.text = element_text(size = textSize),
      legend.key.size = unit(spaceLegend, "lines")
    )
}

# Make the plot
multi_bar <- writing_perceptions_long |>
  ggplot(aes(x = Response, fill = Statements)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(
    title = "Student Self-Perceptions on Writing",
    x = "Response",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom",
  )

multi_bar <- addSmallLegend(multi_bar, pointSize = 0.5, textSize = 10, spaceLegend = 0.8)

multi_bar
```

Students had varied attitudes toward their writing and their writing abilities in general, as we can see from @fig-writing-multibar-1 and fig-writing-multibar-2. ...

```{r fig-coding-multibar-1}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Student Self-Perceptions on Coding, Part 1
#| fig-align: center

# Create a multi-bar plot for coding self-perceptions grouped by response

# Select first three statements
writing_perceptions <- survey_data |>
  select(
    coding_is_easy_for_me,
    i_like_to_code,
    i_believe_it_is_important_to_be_a_good_coder
  )

writing_perceptions_long <- writing_perceptions |>
  pivot_longer(cols = everything(), names_to = "Statements", values_to = "Response") |>
  mutate(
    Statements = case_when(
      Statements == "coding_is_easy_for_me" ~ "\"Coding is easy for me\"",
      Statements == "i_like_to_code" ~ "\"I like to code\"",
      Statements == "i_believe_it_is_important_to_be_a_good_coder" ~ "\"I believe it is important\n to be a good coder\"",
      TRUE ~ Statements
    ),
    Response = case_when(
      Response == "This is a lot like me" ~ "A lot like me",
      Response == "This is very different to me" ~ "Very different from me",
      Response == "This somewhat describes me" ~ "Somewhat describes me",
    )
  ) |>
  filter(!is.na(Response))

# Taken from: https://stackoverflow.com/questions/52297978/decrease-overal-legend-size-elements-and-text
addSmallLegend <- function(myPlot, pointSize = 0.5, textSize = 3, spaceLegend = 0.1) {
  myPlot +
    guides(
      shape = guide_legend(override.aes = list(size = pointSize)),
      color = guide_legend(override.aes = list(size = pointSize))
    ) +
    theme(
      legend.title = element_text(size = textSize),
      legend.text = element_text(size = textSize),
      legend.key.size = unit(spaceLegend, "lines")
    )
}

# Make the plot
multi_bar <- writing_perceptions_long |>
  ggplot(aes(x = Response, fill = Statements)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(
    title = "Student Self-Perceptions of Coding",
    x = "Response",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom",
  )

multi_bar <- addSmallLegend(multi_bar, pointSize = 0.5, textSize = 10, spaceLegend = 0.8)

multi_bar
```

```{r fig-coding-multibar-2}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Student Self-Perceptions on Coding, Part 2
#| fig-align: center

# Create a multi-bar plot for coding self-perceptions grouped by response

# Select first three statements
writing_perceptions <- survey_data |>
  select(
    when_i_check_my_code_it_is_easy_for_me_to_catch_my_mistakes,
    i_feel_confident_sharing_my_code,
    i_am_confident_in_my_overall_coding_ability
  )

writing_perceptions_long <- writing_perceptions |>
  pivot_longer(cols = everything(), names_to = "Statements", values_to = "Response") |>
  mutate(
    Statements = case_when(
      Statements == "when_i_check_my_code_it_is_easy_for_me_to_catch_my_mistakes" ~ "\"When I check my code, it is\n easy for me to catch my mistakes\"",
      Statements == "i_feel_confident_sharing_my_code" ~ "\"I feel confident sharing my code\"",
      Statements == "i_am_confident_in_my_overall_coding_ability" ~ "\"I am confident in\n my overall coding ability\"",
      TRUE ~ Statements
    ),
    Response = case_when(
      Response == "This is a lot like me" ~ "A lot like me",
      Response == "This is very different to me" ~ "Very different from me",
      Response == "This somewhat describes me" ~ "Somewhat describes me",
    )
  ) |>
  filter(!is.na(Response))

# Taken from: https://stackoverflow.com/questions/52297978/decrease-overal-legend-size-elements-and-text
addSmallLegend <- function(myPlot, pointSize = 0.5, textSize = 3, spaceLegend = 0.1) {
  myPlot +
    guides(
      shape = guide_legend(override.aes = list(size = pointSize)),
      color = guide_legend(override.aes = list(size = pointSize))
    ) +
    theme(
      legend.title = element_text(size = textSize),
      legend.text = element_text(size = textSize),
      legend.key.size = unit(spaceLegend, "lines")
    )
}

# Make the plot
multi_bar <- writing_perceptions_long |>
  ggplot(aes(x = Response, fill = Statements)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(
    title = "Student Self-Perceptions of Coding",
    x = "Response",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom",
  )

multi_bar <- addSmallLegend(multi_bar, pointSize = 0.4, textSize = 8.5, spaceLegend = 0.8)

multi_bar
```

Student self-perceptions about their coding abilities, shown in @fig-coding-multibar-1 and @fig-coding-multibar-2 were also very interesting. ...

```{r fig-ai-appropriate-pie}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Comparison of Whether AI Use is Appropriate in School
#| fig-align: center

# Pie chart of student opinion on appropriateness of AI use in class.

appropriate <- survey_data |>
  select(ai_schoolwork_appropriate) |>
  count(ai_schoolwork_appropriate)

ggplot(appropriate, aes(x = "", y = n, fill = ai_schoolwork_appropriate)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  labs(
    title = "Do students think AI use is appropriate in class?",
    x = NULL,
    y = NULL
  ) +
  theme_minimal() +
  scale_fill_discrete(name = "Response") +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(hjust = 0.5),
    axis.text.x = element_blank(), # Remove x-axis text
    axis.title.x = element_blank(),
  ) +
  geom_text(aes(label = n),
    position = position_stack(vjust = 0.5)
  )
```

## Student Usage 

```{r fig-ai-use-multibar}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Hello
#| fig-align: center

# Create a multi-bar plot for AI use/perceptions grouped by response

# THROWS ERROR: FIX

# Select first three statements
writing_perceptions <- survey_data |>
  select(
    used_ai_any_reason,
    used_ai_in_sta302
  )

writing_perceptions_long <- writing_perceptions |>
  pivot_longer(cols = everything(), names_to = "Statements", values_to = "Response") |>
  mutate(
    Statements = case_when(
      Statements == "used_ai_any_reason" ~ "Used AI for any reason",
      Statements == "used_ai_in_sta302" ~ "Used AI in Class",
      TRUE ~ Statements
    ),
  ) |>
  filter(!is.na(Response)) |>
  filter(Response == "No" | Response == "Yes")

# Taken from: https://stackoverflow.com/questions/52297978/decrease-overal-legend-size-elements-and-text
addSmallLegend <- function(myPlot, pointSize = 0.5, textSize = 3, spaceLegend = 0.1) {
  myPlot +
    guides(
      shape = guide_legend(override.aes = list(size = pointSize)),
      color = guide_legend(override.aes = list(size = pointSize))
    ) +
    theme(
      legend.title = element_text(size = textSize),
      legend.text = element_text(size = textSize),
      legend.key.size = unit(spaceLegend, "lines")
    )
}

# Make the plot
multi_bar <- writing_perceptions_long |>
  ggplot(aes(x = Response, fill = Statements)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(
    title = "Student Self-Perceptions of Coding",
    x = "Response",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(hjust = 0.5),
  )

multi_bar <- addSmallLegend(multi_bar, pointSize = 0.5, textSize = 10, spaceLegend = 0.8)

multi_bar
```

```{r fig-AI-usage-type-comp}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: How Students Use ChatGPT
#| fig-align: center

# Create a bar plot of how students used ChatGPT - FIX: sort from largest to smallest

type_data <- survey_data |>
  select(
    not_used,
    used_technical_q,
    used_conversation,
    used_general_knowledge,
    used_solving_hmk,
    used_check_soln,
    used_quick_q,
    used_expl_concepts,
    used_writing_content,
    used_writing_code
  ) |>
  pivot_longer(cols = everything(), names_to = "Statements", values_to = "Response") |>
  mutate(Statements = case_when(
    Statements == "not_used" ~ "Not Used",
    Statements == "used_technical_q" ~ "Technical Questions",
    Statements == "used_conversation" ~ "Fun Conversation",
    Statements == "used_general_knowledge" ~ "General Knowledge",
    Statements == "used_solving_hmk" ~ "Entirely Solving Homework",
    Statements == "used_check_soln" ~ "Checking my Solutions",
    Statements == "used_quick_q" ~ "Quick Questions",
    Statements == "used_expl_concepts" ~ "Explaining Concepts",
    Statements == "used_writing_content" ~ "Writing Paper Content",
    Statements == "used_writing_code" ~ "Writing Paper Code",
    TRUE ~ Statements
  )) |>
  filter(Response == 1) |>
  count(Statements) |>
  arrange(desc(n))

type_data |>
  ggplot(aes(x = n, y = reorder(Statements, n))) +
  geom_bar(stat = "identity", fill = "grey50") +
  labs(title = "How Students Used ChatGPT", x = "Count", y = "Use Type") +
  theme_classic()
```

To understand the role of AI chatbots in learning, students identified their usage by checking off various pre-defined use cases in the survey. @fig-AI-usage-type-comp demonstrates that asking technical questions and explaining concepts were the two top use cases among students. Less than half used it to entirely solve their homework, which could be explained by the complex nature of the homework, that involved doing extensive research and writing. Also just less than half used it to write paper content, which could suggest that students do not feel confident using it to improve writing, concurring with student attitudes in @Cahill_2024.

TODO: Make a viz of helpfulness in various assignments. Describe the context of the assignment. 

```{r fig-AI-helpfulness}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: CHANGE/REMOVE
#| fig-align: center


```

## Sentiment Analysis

```{r setup-sentiment}
#| message: false
#| echo: false
#| warning: false

#get positive words from NRC lexicon
neg_words <- get_sentiments("bing") |>
  filter(sentiment == "negative")

#get negative words from NRC lexicon
pos_words <- get_sentiments("bing") |>
  filter(sentiment == "positive")
```

We then performed sentiment analysis on the free-response comment questions asked to students. To perform this text analysis, the tidytext package [@tidytext] was used. 

### Appropriateness of AI in Schoolwork 

In the survey, students were asked to respond to the question: "Please elaborate on to what extent do you think using generative AI tools such as ChatGPT by OpenAI (or equivalents) is ethical and appropriate for schoolwork?". 

```{r setup-ai-appropriate}
#| message: false
#| echo: false
#| warning: false

# AI Appropriateness Comments
ai_comments <- survey_data |>
  select(
    id,
    ai_schoolwork_appropriate_comments
  ) |>
  mutate(
    comment = ai_schoolwork_appropriate_comments
  ) |>
  select(-c(ai_schoolwork_appropriate_comments))

# put comments into tidytext format
tidy_ai_comments <- ai_comments |>
  group_by(id) |>
  unnest_tokens(word, comment)

```

```{r tbl-ai-appropriate}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Comparing Incidences of Positive and Negative Words.
#| tbl-subcap: ["Top 10 Mentioned Positive Words", "Top 10 Mentioned Negative Words"]
#| layout-ncol: 2
#| fig-align: center


# Generate positive words table
tbl_ai_pos <- tidy_ai_comments |>
  ungroup() |>
  select(-c("id")) |>
  inner_join(pos_words) |>
  count(word, sort = TRUE) |>
  head(10) |>
  kable(, format = "markdown")

# Generate negative words table
tbl_ai_neg <- tidy_ai_comments |>
  ungroup() |>
  select(-c("id")) |>
  inner_join(neg_words) |>
  count(word, sort = TRUE) |>
  head(10) |>
  kable(, format = "markdown") 

tbl_ai_pos
tbl_ai_neg

```

```{r fig-wordcloud-ai-appropriate}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: (CHANGE) Boilerplate figure caption 
#| fig-align: center

# Load sentiment lexicon
sentiments <- get_sentiments()

# Prepare data for comparison cloud
comparison_data <- tidy_ai_comments |>
  inner_join(sentiments) |>
  count(word, sentiment, sort = TRUE) |>
  acast(word ~ sentiment, value.var = "n", fill = 0)

# Generate comparative word cloud
comparison.cloud(
  comparison_data,
  colors = c("darkred", "darkgreen"),
  max.words = 100,
  title.size = 1.5,
  scale = c(3, 0.5)  # Adjust scale for word size
)



```


### Recommendations in STA302 
```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Hello
#| fig-align: center

recs_comments <- survey_data |>
  select(
    id,
    recs_for_ai_sta302
  )
```

# Model {#sec-model}

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.

\begin{align}
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results {#sec-results}

Our results are summarized in @tbl-modelresults.







# Discussion {#sec-discussion}

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In ... we implement a posterior predictive check. This shows...

In ... we compare the posterior with the prior. This shows...


## Diagnostics

... is a trace plot. It shows... This suggests...

... is a Rhat plot. It shows... This suggests...



\newpage


# References


