---
title: "Self-reported LLM usage and results on a data science project: Evidence from a Canadian undergraduate data science course"
author: 
  - Rohan Alexander
  - Luca Carnegie
  - Nathalie Moon
thanks: "Code and data are available at: https://github.com/lcarnegie/llms-achievement. We thank Nathalie Moon for developing and sharing her survey questions. We thank Tiffany Timbers and attendees at JSM 2024 for helpful suggestions. This research is currently under review by the University of Toronto's University Research Ethics Board. Comments can be sent to: rohan.alexander@utoronto.ca."
date: today
date-format: long
abstract: "To help understand the effect of Large Language Models (LLMs) on data science practice we examine the extent to which LLM usage is correlated with the mark that a student gets on a final paper in a classroom data science setting. We find some very mild evidence from this observational study that LLM usage may be associated with better scores, but our main conclusion is that there is no clear relationship between more extensive LLM usage and the student's mark. Despite the classroom setting used for evaluation, the particular activity of interest reflects the work done by professional data scientists. Our finding suggests the need for more extensive work evaluating how LLMs can be integrated into the data science workflow in a way that provides value."
format: pdf
number-sections: true
table-of-contents: false
bibliography: references.bib
---

```{r setup}
#| include: false

library(knitr)
library(quanteda)
library(readxl)
library(reshape2)
library(tidytext)
library(tidyverse)
library(tinytex)

# Read in data
all_data <-
  read_excel(here::here("data/merged_data.xlsx"), sheet = "Sheet1") |>
  janitor::clean_names()
```

# Introduction

There are many aspects of trustworthy data science, including education, culture, and workflow to name a few. One aspect of special importance is the tools that we use to do data science: the computers, programming languages, and environments. The wide release of user-friendly Large Language Models (LLMs), particularly OpenAI's ChatGPT, is a rare instance of a new and powerful tool being made available to add to a data scientist's toolbelt.

Like all new tools, however, LLMs have created both excitement and apprehension. ChatGPT's public release on November 30, 2022, brought LLMs into mainstream conversation, quickly being put to use in both industry and academia. By now, many people, especially educators and students, have some experience with LLMs, in both personal and professional contexts.

In the context of teaching statistics and data science, LLMs could be useful for many tasks. For instance, there is considerable interest in the potential of chatbots to act as personalized tutors for students [@Winkler_2018]. ChatGPT has also been the catalyst for many interesting and important conversations around academic integrity [@Okaibedi_2023], the development of critical thinking skills, and what effective learning looks like [@BaidooAnu_2023].

In this paper we are interested in better understanding LLMs as a tool for producing trustworthy data science. We study how they were used by students in an upper-year undergraduate data science course and whether students who used LLMs tended to have higher scores than those who did not.

The tasks involved in a trustworthy data science workflow can be generally broken down into a number of key competencies [@Adhikari2021Interleaving; @Gibbs2021Building]. The first is programming, which is done when cleaning, analyzing, visualizing data often using languages like R or Python. The second is writing, which is primarily done when communicating results. The potential for LLMs to positively affect students' academic performance in data science can be clearly inferred from already-demonstrated effects in a number of those competencies on adjacent professional fields.

In terms of computer programming, @Peng_2023 found a positive impact of GitHub Copilot (an LLM-powered programming assistant) on productivity. Specifically, in an experiment involving 95 freelance programmers, they found that that programmers with access to GitHub Copilot completed a standardized programming task 56% faster than the control group. Programmers with less experience saw the greatest improvements in productivity.

@DellAcqua_2023, focusing on management consulting tasks, provide evidence that LLMs can improve writing productivity. In a field experiment involving consultants from Boston Consulting Group they found that the use of OpenAI's GPT-4 led to a 25% increase in delivery speed of business tasks (most involving some writing), as well as a 40% increase in human-rated performance on those tasks. Similar to computer programming, these productivity increases were most pronounced for those with below average performance, with their output increasing by 43%.

On the other hand, @Valenzuela_2024 argue that LLMs lead to a loss of serendipity (which leads to less original work) and de-skilling (primarily with respect to programming ability), among other consequences. These outcomes could negatively affect students' effective learning of data science. 

@Ellis_Slade_2023 take a more optimistic perspective and argue that LLMs are just another technology that will impact statistics and data science education. Similarly, @Tu_2024 acknowledge that LLMs can streamline many parts of a data science workflow. With that in mind, they suggest that data-scientists-in-training should shift their self-perspectives from primarily being an 'analyst' to primarily being a 'product manager' responsible for strategic oversight of the analysis carried out by LLMs. 

At the high school level @Lazar_2023 conducted a informal survey of secondary school teachers and students on their opinions of ChatGPT, and found that while LLMs could: help creativity, provide academic support when teachers were unavailable, and model certain types of writing well; teachers were also aware of LLMs potential to limit students' learning in certain ways through over-reliance. Beyond academic integrity concerns, teachers had similar concerns to @Valenzuela_2024 about de-skilling and an overall loss of agency in writing and critical thinking.

@Cahill_2024 surveyed undergraduate political science students on their attitudes toward and usage of AI tools. They found that the use of ChatGPT was widespread. However, they also found that many students lacked confidence in using AI for academic purposes. In particular, only 11% 'strongly agreed' that they know how to use AI to improve their writing. Students had nuanced views on appropriate AI use. In particular, respondents found that using it to write whole papers as inappropriate, while using it for basic tasks like general assistance, writing feedback and basic data visualization was perceived more appropriately.

To understand the current state of LLMs as a tool for trustworthy data science, this paper focuses on the association between student academic performance and their LLM usage. Specifically, we examine the relationship between students' grades and self-reported measures of student LLM usage, as well as student attitudes toward LLMs in general. This is based on students' final papers and a survey, conducted in a third-year undergraduate data science course at the University of Toronto. By examining how students interact with and perceive LLMs as tools, and how these variables translate into student outcomes, the effects of LLM integration in data science can be more precisely determined, leading to better recommendations for their future development.

The remainder of this paper is structured as follows: @sec-data visualizes and analyzes survey data and coursework from students. @sec-model specifies a model used to investigate the relationship. @sec-results describes and analyzes the model's results. @sec-discussion discusses the implications of the findings for data science education and future research and practice at the intersection of LLMs and trustworthy data science.

# Data {#sec-data}

## Background

To investigate students' usage and attitudes towards LLMs and how they related to their academic performance, a dataset containing their usage/attitudes, coursework, and academic performance was constructed. This was based on three components: 

1. an optional survey; 
2. self-reported LLM usage; and 
3. student marks on their final paper.

All data are from the cohort of students taking STA302 "Methods of Data Analysis I" in the Winter 2024 semester at the University of Toronto. This course had 275 students initially enrolled which, reflecting a normal rate of attrition for undergraduate statistics courses at the University of Toronto, reduced to 154 students by the end of the semester. Assessment was heavily based on three papers submitted over the course of the 12 week semester. 

The student marks that we analyze are based only on the final paper, which is done individually. By this stage, uninterested students have typically dropped the course, and students are familiar with course expectations. A typical paper submission is 10-20 pages, and requires students to conduct original research to answer a research question of interest to them. It reflects the skills typically used by a professional data scientist. Students are expected to develop a research question of interest to them, identify or collect data to answer the question, conduct statistical analysis, and write a short paper. Examples of final papers (shared with consent) include: @hannahyu; @emilysu; and @bennyrochwerg.

By the time they are working on their final paper, students have submitted and received feedback on two previous papers with similar requirements and rubrics to that of the final paper. Each paper has the same basic structure and expectations. Before the final paper is due, students have received feedback on all their previous work in the class (including their past papers) and there is an optional two-day period of peer review.

By virtue of pre-requisites of this course the typical student is an upper-year undergraduate. Coding and writing are major parts of the course. Students are welcome to use R or Python, but the majority code in R because that is the programming language currently mostly taught in pre-requisite courses. All writing must be in English. The primary aim of the course for students is to create a public portfolio of work they can use to apply for jobs.

Throughout the semester students were encouraged to use LLMs. Formal instruction was provided twice during the semester, with a masterclass taught by a computer science faculty member on the ethics of using LLMs (see @Horton2024 for details), and another masterclass taught by a TA on writing with LLMs. 

Data was collected from students through an optional end-of-course survey. [Appendix -@sec-survey-details] provides all the questions asked in the survey. Whether or not they consented to their data being used, all respondents received a 1% increase in their final course grade for their participation. Consenting responses were then matched to their final paper mark, as well as the GitHub repository for their final paper. The responses were anonymized by removing any personal references to the students themselves including names, emails, student numbers, and GitHub links.

All data cleaning and analysis was done using the `R` statistical programming language [@citeR], and especially uses the `tidyverse` [@tidyverse], `janitor` [@janitor], `reshape2` [@reshape], and `readxl` [@readxl] packages. 

## Survey data

There were 146 responses to the survey. Of these, 119 respondents provided authorization for their data to be collected and used. Four of those respondents submitted the survey twice, and after removing their second response, 115 responses remained. Of those, 15 respondents did not include a statement on LLM usage in the README of the GitHub repository of their final paper, leaving 100 responses that were of use and were merged based on student name.

All but one respondent completed the survey within 45 minutes (@fig-response-time-and-gpa-1). That one respondent took more than 4,000 minutes to complete the survey, suggesting they took a break while filling it out. That respondent is included in the analysis dataset, but without that respondent, the average time to complete the survey was 11 minutes, and the standard deviation was 8 minutes.

```{r fig-response-time-and-gpa}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Distribution of respondents' survey response times and GPA
#| fig-subcap: ["Distribution of survey response times","\"What is your GPA?\""]
#| layout-ncol: 1

all_data |>
  mutate(mins_complete = completion_time - start_time) |>
  filter(mins_complete < 1000) |> # Remove one student who took 4,000 minutes to complete
  # summarize(mean = mean(mins_complete),
  # standard_dev = sd(mins_complete))
  ggplot(aes(x = mins_complete)) +
  geom_histogram(binwidth = 1) +
  labs(x = "Time to complete survey (minutes)", y = "Frequency") +
  theme_minimal()

all_data |>
  # summarize(mean = mean(what_is_your_gpa, na.rm = TRUE),
  #           standard_dev = sd(what_is_your_gpa, na.rm = TRUE))
  ggplot(aes(x = what_is_your_gpa)) +
  geom_histogram(binwidth = 0.1) +
  labs(x = "Self-reported GPA", y = "Frequency") +
  theme_minimal()
```

There is a wide distribution of self-reported GPAs (@fig-response-time-and-gpa-2). Seven of the 100 respondents included in the analysis dataset did not report their GPA. The majority of responses cluster around a B (3.0/4.0), and the average is 3.06 with a standard deviation of 0.55. One factor that may affect the range is that the course is required for programs in the Statistics, Mathematics and Computer Science Departments. Self-reported GPAs also introduce the possibility of reporting bias. For instance, respondents may have provided their cumulative GPA, their most recent term's GPA, or could have misreported it entirely.

Students from a range of years took the course, however the majority of respondents were in their 3rd or 4th year of study (@tbl-year-dist). The course's prerequisite of general statistics, which is a two-course sequence typically completed by students in their second year, would make it difficult to take this course earlier than their 3rd year.

```{r tbl-year-dist}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: \"What year are you?\"

all_data |>
  mutate(
    what_year_are_you = case_when(
      what_year_are_you == "PEY" ~ "4th",
      # Change PEY to 4th year
      what_year_are_you == "5th" ~ "5th or over",
      what_year_are_you == "6th" ~ "5th or over",
      TRUE ~ what_year_are_you
    )
  ) |>
  count(what_year_are_you) |>
  pivot_wider(
    names_from = what_year_are_you,
    values_from = n,
    values_fill = list(n = 0)
  ) |>
  kable()
```

Respondents had a varied self-perception of their coding and writing abilities (@fig-selfperception). Most respondents believed it is important to be good at writing, but many are either indifferent or do not like to write (@fig-selfperception-1). Respondents also do not find writing to be particularly easy, which could be associated with the reported relative antipathy toward writing. Most respondents were at least somewhat confident in their own writing abilities, but a substantial contingent felt otherwise. Although few respondents felt that they were confident in their writing ability, more felt that they were able to catch their mistakes, which could indicate a disconnect between how respondents perceive their work and how the work was evaluated.

```{r fig-selfperception}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Self-perception of coding and writing abilities
#| fig-subcap: ["\"Please rate how much each statement describes you, on a scale from 'This is very different to me' to 'This is a lot like me'\"", "\"Please rate how much each statement describes you, on a scale from 'This is very different to me' to 'This is a lot like me'\""]

# Writing
writing_perceptions_long <-
  all_data |>
  select(
    writing_is_easy_for_me,
    i_like_to_write,
    i_believe_it_is_important_to_be_a_good_writer,
    when_i_edit_it_is_easy_for_me_to_catch_my_mistakes,
    i_feel_confident_sharing_my_writing,
    i_am_confident_in_my_overall_writing_ability
  ) |>
  pivot_longer(cols = everything(),
               names_to = "statements",
               values_to = "response") |>
  mutate(
    statements = case_when(
      statements == "writing_is_easy_for_me" ~ "\"Writing is easy for me.\"",
      statements == "i_like_to_write" ~ "\"I like to write.\"",
      statements == "i_believe_it_is_important_to_be_a_good_writer" ~ "\"I believe it is important\nto be a good writer.\"",
      statements == "when_i_edit_it_is_easy_for_me_to_catch_my_mistakes" ~ "\"When I edit it is easy for\nme to catch my mistakes.\"",
      statements == "i_feel_confident_sharing_my_writing" ~ "\"I feel confident\nsharing my writing.\"",
      statements == "i_am_confident_in_my_overall_writing_ability" ~ "\"I am confident in my\noverall writing ability.\"",
      TRUE ~ statements
    ),
    response = case_when(
      response == "\"This is a lot like me\"" ~ "A lot like me",
      response == "\"This is very different to me\"" ~ "Very different from me",
      response == "\"This somewhat describes me\"" ~ "Somewhat describes me",
      TRUE ~ response
    )
  )

writing_perceptions_long |>
  ggplot(aes(x = response, fill = response)) +
  geom_bar(position = "stack", stat = "count") +
  facet_wrap(vars(statements)) +
  labs(x = "Response", y = "Number of repondents", fill = "Response:") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), legend.position = "bottom")

# Coding
coding_perceptions_long <-
  all_data |>
  select(
    coding_is_easy_for_me,
    i_like_to_code,
    i_believe_it_is_important_to_be_a_good_coder,
    when_i_check_my_code_it_is_easy_for_me_to_catch_my_mistakes,
    i_feel_confident_sharing_my_code,
    i_am_confident_in_my_overall_coding_ability
  ) |>
  pivot_longer(cols = everything(),
               names_to = "statements",
               values_to = "response") |>
  mutate(
    statements = case_when(
      statements == "coding_is_easy_for_me" ~ "\"Coding is easy for me\"",
      statements == "i_like_to_code" ~ "\"I like to code\"",
      statements == "i_believe_it_is_important_to_be_a_good_coder" ~ "\"I believe it is important\n to be a good coder\"",
      statements == "when_i_check_my_code_it_is_easy_for_me_to_catch_my_mistakes" ~ "\"When I check my code, it is\n easy for me to catch my\nmistakes\"",
      statements == "i_feel_confident_sharing_my_code" ~ "\"I feel confident\n sharing my code\"",
      statements == "i_am_confident_in_my_overall_coding_ability" ~ "\"I am confident in\n my overall coding ability\"",
      TRUE ~ statements
    ),
    response = case_when(
      response == "\"This is a lot like me\"" ~ "A lot like me",
      response == "\"This is very different to me\"" ~ "Very different from me",
      response == "\"This somewhat describes me\"" ~ "Somewhat describes me",
      TRUE ~ response
    )
  )

coding_perceptions_long |>
  ggplot(aes(x = response, fill = response)) +
  geom_bar(position = "stack", stat = "count") +
  facet_wrap(vars(statements)) +
  labs(x = "Response", y = "Number of repondents", fill = "Response:") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), legend.position = "bottom")
```

Respondent self-perceptions regarding coding proficiency and importance varied (@fig-selfperception-2). There was strong consensus on the perceived importance of coding skills, with a majority of respondents strongly affirming this belief. However, respondents' self-assessed coding abilities and enjoyment are more heterogeneous, with a substantial proportion reporting moderate rather than high levels of ease and enjoyment in coding tasks. These findings suggest a complicated relationship between respondents' recognition of coding's importance and their personal experience with coding.

Overall there was a moderate level of confidence among respondents in their overall coding ability, willingness to share code, and capacity to identify errors (@fig-selfperception-2). Notably, respondents express slightly higher confidence in detecting their own coding mistakes compared to general coding ability or code sharing. These patterns suggest that while respondents have developed some coding self-efficacy, there is still considerable potential for enhancing their perceived competence and comfort across various coding-related activities.

The majority of respondents were at least "somewhat familiar" with generative AI such as ChatGPT (@tbl-ai-familiarity-usage-1). Though respondents' self-perceptions around writing and coding are varied, there was strong consensus that the use of generative AI tools is appropriate within an academic setting (@tbl-ai-familiarity-usage-2). Most respondents who responded "It depends" generally found artificial intelligence tools to be appropriate, though with certain guidelines and rules governing their use.

```{r tbl-ai-familiarity-usage}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Familiarity with, and appropriateness of, generative AI
#| tbl-subcap: ["'How familiar are you with using generative AI tools such as OpenAI's ChatGPT or equivalents?'","'To what extent do you think using generative AI tools such as ChatGPT by OpenAI (or equivalents) is ethical and appropriate for schoolwork?'"]

all_data |>
  rename(ai_familiarity = how_familiar_are_you_with_using_generative_ai_tools_such_as_open_a_is_chat_gpt_or_equivalents) |>
  mutate(
    ai_familiarity = case_when(
      ai_familiarity == "Limited to some liberal arts, such as history courses" ~ "Somewhat familiar",
      TRUE ~ ai_familiarity
    )
  ) |>
  count(ai_familiarity) |>
  kable(col.names = c("Extent of AI familiarity", "Students"))


all_data |>
  rename(ai_schoolwork_appropriate = to_what_extent_do_you_think_using_generative_ai_tools_such_as_chat_gpt_by_open_ai_or_equivalents_is_ethical_and_appropriate_for_schoolwork) |>
  mutate(
    ai_schoolwork_appropriate = case_when(
      ai_schoolwork_appropriate == "Depends of what the main goal of schoolwork is" ~ "It depends",
      ai_schoolwork_appropriate == "I believe student can only use it if they are allowed to" ~ "It depends",
      ai_schoolwork_appropriate == "a controllable amount" ~ "It depends",
      ai_schoolwork_appropriate == "appropriate depending on the context" ~ "It depends",
      ai_schoolwork_appropriate == "appropriate with rules for usage" ~ "It depends",
      ai_schoolwork_appropriate == "depends" ~ "It depends",
      ai_schoolwork_appropriate == "depends on how it is used" ~ "It depends",
      ai_schoolwork_appropriate == "half and half" ~ "It depends",
      ai_schoolwork_appropriate == "unsure" ~ "It depends",
      TRUE ~ ai_schoolwork_appropriate
    )
  ) |>
  count(ai_schoolwork_appropriate) |>
  kable(col.names = c("Ethical & Appropriate for school?", "Students"))
```

To understand the role of LLMs in learning, students identified their usage in a more granular way by checking off various pre-defined use cases in the survey (@fig-AI-use-and-usefulness-1). Technical questions and explaining concepts were the two top use cases among students in the course. More than half of students also used LLMs for quick questions, general knowledge, writing paper code, and checking solutions. Just less than half used it to write paper content, which could suggest that students do not feel confident using it to improve writing.

```{r fig-AI-use-and-usefulness}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: "Use and usefulness of generative AI"
#| fig-subcap: ["\"If you have used generative AI tools such as OpenAI's ChatGPT or equivalents, in what ways have you used it (select all that apply)?\"", "\"How helpful did you find generative AI tools such as ChatGPT by OpenAI (or equivalents) for each component of STA302?\""]

# Use
all_data |>
  rename(how_have_you_used = if_you_have_used_generative_ai_tools_such_as_open_a_is_chat_gpt_or_equivalents_in_what_ways_have_you_used_it_select_all_that_apply) |>
  select(how_have_you_used) |>
  separate_rows(how_have_you_used, sep = ";") |>
  mutate(
    how_have_you_used = case_when(
      how_have_you_used == "check grammar mistake" ~ "Other",
      how_have_you_used == "debugging code" ~ "Other",
      how_have_you_used == "mental health support" ~ "Other",
      how_have_you_used == "Making practice questions for me with increasing difficulty" ~ "Other",
      how_have_you_used == "Generating test questions" ~ "Other",
      TRUE ~ how_have_you_used
    )
  ) |>
  count(how_have_you_used) |>
  filter(how_have_you_used != "") |>
  arrange(desc(n)) |>
  ggplot(aes(x = n, y = reorder(how_have_you_used, n))) +
  geom_bar(stat = "identity") +
  labs(x = "Number of respondents", y = "Type of usage") +
  theme_minimal()


# Usefulness
all_data |>
  select(
    weekly_quiz,
    weekly_mini_essay,
    papers_generating_ideas,
    papers_writing_code,
    papers_writing_content,
    papers_improving_content
  ) |>
  pivot_longer(cols = everything(),
               names_to = "task",
               values_to = "response") |>
  mutate(
    # Simplify response categories and rename tasks
    response_simplified = case_when(
      response %in% c(
        "\"Not helpful\"",
        "\"I did not use generative AI for this component\""
      ) ~ "Less Helpful",
      response %in% c("\"Somewhat helpful\"", "\"Very helpful\"") ~ "More Helpful",
      TRUE ~ "Unknown"  # Handle unexpected values
    ),
    task = case_when(
      task == "weekly_quiz" ~ "Weekly Quiz",
      task == "weekly_mini_essay" ~ "Weekly Mini Essay",
      task == "papers_generating_ideas" ~ "Generating Ideas",
      task == "papers_writing_code" ~ "Writing Code",
      task == "papers_writing_content" ~ "Writing Content",
      task == "papers_improving_content" ~ "Improving Content",
      TRUE ~ "Other"  # Handle any unexpected task names
    )
  ) |>
  group_by(task, response_simplified) |>
  summarise(count = n(), .groups = 'drop') |>
  ggplot(aes(x = response_simplified, y = count, fill = response_simplified)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ task, scales = "free_y") +
  labs(y = "Number of respondents", x = "", fill = "Response type: ") +
  theme_minimal() +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  theme(axis.text.y = element_blank(), legend.position = "bottom")
```

Respondents were also asked to rate the helpfulness of LLMs on various tasks assigned during the course on a 4-point scale of "Did not use" to "Very Helpful" (@fig-AI-use-and-usefulness-2). To simplify the presentation, responses were grouped into two main categories: "Less Helpful" and "More Helpful." The "Less Helpful" category combines responses where students found the AI either "Not helpful" or did not use it for the task, while the "More Helpful" category includes responses where the AI was considered "Somewhat helpful" or "Very helpful".

Respondents differed in terms of how they used LLMs in the course (@fig-AI-use-and-usefulness-2). While most tasks were roughly split between respondents finding LLMs helpful or not, respondents found them most helpful in generating code. In the context of the course, this meant generating R code for transforming, analyzing, and visualizing data. To a lesser extent, respondents also found LLMs to be helpful in improving the existing writing they had, while at the same time not favouring it for writing content from scratch. 

Finally, one question asked students to elaborate on whether they thought generative AI tools such as ChatGPT were ethical and appropriate for schoolwork. This was an open-response question. To provide a sense of the responses, we used Anthropic's Claude 3.5 Sonnet model (as at 5 August 2024) to summarize the comments and it provided:

> Many students view AI as a helpful supplementary tool, comparing it to resources like Google or calculators. They believe it can aid in understanding concepts, debugging code, brainstorming ideas, and saving time on routine tasks. However, there's a consensus that AI should not be used to complete entire assignments or replace original thinking. Students emphasize the importance of using AI ethically, citing it when appropriate, and not relying on it exclusively. Some argue that learning to use AI effectively is a valuable skill for future careers. Concerns raised include the potential for plagiarism, the risk of hindering critical thinking skills, and the possibility of receiving incorrect information. Overall, most students support the responsible use of AI in education, with proper guidelines and transparency, while recognizing the need to maintain academic integrity and develop independent learning skills.


## LLM usage and final paper marks

Two other components were merged with the survey responses: self-reported LLM usage on the final paper, and final paper mark.

Students were encouraged to use LLMs to complete their papers. Each paper required the students to disclose their usage through a statement in the GitHub repository README for the paper. Even students who did not use generative AI at all were required to state this in the README. For students who did use generative AI, there was an additional requirement that they save the logs of their usage in a txt file which was also included in their GitHub repository.

Those README statements were gathered and parsed using OpenAI's ChatGPT 4o model (as at 26 July). The following prompt was used: 'The following statement is about to what extent LLMs were used by a student. Please characterize it as one of: "None", "Minimal", "Somewhat", "Extensive", "Unsure". Respond with only one of those options.' All classifications were then manually checked by hand for reasonableness.

We find a varied extent of self-reported LLM usage (@tbl-llm-usage). 41 respondents were classified as having made extensive use of LLMs, while 28 were classified as having made somewhat use. 31 respondents were classified as having made minimal or no use of LLMs. In the analysis dataset we combine those two classifications because only 8 respondents were classified as having minimal usage.

```{r tbl-llm-usage}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Self-reported LLM usage in final paper

all_data |>
  count(llm_usage) |> 
  kable(col.names = c("Self-reported LLM usage", "Number"))
```

The third, and final, component is the mark, in percentages, on the final paper (@fig-mark). The overall mean was 78% and standard deviation was 17 percentage points. However there was considerable differences by the extent of LLM usage. For extensive use, the mean was 82% and the standard deviation was 15 percentage points. For somewhat usage, the mean was 77% and the standard deviation was 16 percentage points. For minimal usage, the mean was 74% and the standard deviation was 23 percentage points. Finally for no usage, the mean was 75% and the standard deviation was 19 percentage points.

```{r fig-mark}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Mark on final paper, in percentages, and LLM usage in final paper

# all_data |>
#   group_by(llm_usage) |>
#   summarize(mean = mean(mark) |> round(2),
#             sd = sd(mark) |> round(2))
#
# all_data |>
#   summarize(mean = mean(mark) |> round(2),
#             sd = sd(mark) |> round(2))

all_data |>
  ggplot(mapping = aes(x = mark, color = )) +
  geom_histogram(binwidth = 0.01) +
  scale_x_continuous(labels = scales::label_percent()) +
  labs(x = "Final paper mark (percentage)", y = "Count") +
  facet_wrap(vars(llm_usage),
             ncol = 2,
             labeller = labeller(
               llm_usage = c(
                 "Extensive" = "Extensive LLM usage",
                 "Minimal" = "Minimal LLM usage",
                 "None" = "No LLM usage",
                 "Somewhat" = "Some LLM usage"
               )
             )) +
  theme_minimal()
```

# Model {#sec-model}

The goal of our modelling strategy is to better understand how respondents result on their final paper is associated with their self-reported LLM usage. The result on the final paper is a proportion and some respondents got full marks and so we use zero-one-inflated beta regression. Here we briefly describe the model that we use, which follows @kurztotherescue. Diagnostics are included in [Appendix -@sec-model-details].

Define $y_i$ as the percentage received on the final paper. Then $\beta_1$ is the characterization of self-reported LLM usage and $\beta_2$ is self-reported GPA.

\begin{align}
y_i &\sim \mbox{Beta}(\mu_i, \phi) \\
logit(\mu_i) &= \beta_0 + \beta_1 \times \mbox{LLM usage}_i + \beta_2 \times \mbox{GPA}_i\\
\beta_0, \beta_1, \beta_2  &\sim \mbox{Normal}(0, 1) \\
\phi &\sim \mbox{Gamma}(4, 0.1)
\end{align}

We estimate the model in `R` [@citeR] using `brms` [@brms].

The expected relationship between LLM usage and final mark is unclear. It may be that stronger students used LLMs in a more sophisticated way or that they did not need to use them. However, the relationship between GPA and final mark is expected to be positive.

# Results {#sec-results}

Our results are summarized in @tbl-model-results and @fig-model-results. We especially draw on `modelsummary` [@modelsummary].

```{r tbl-model-results}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Coefficient estimates and mean absolute deviation (MAD)

fit1 <-
  readRDS(file = here::here("models/fit1brms.rds"))
fit2 <-
  readRDS(file = here::here("models/fit2brms.rds"))

modelsummary::modelsummary(
  list(
    "Base model" = fit1,
    "Including self-reported GPA" = fit2
  ),
  statistic = "mad",
  fmt = 2
)
```


```{r fig-model-results}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Coefficient estimates and 90 per cent credibility intervals

modelsummary::modelplot(
  list(
    "Base model" = fit1,
    "Including self-reported GPA" = fit2
  ),
  conf_level = 0.9,
  coef_omit = "phi") +
  labs(x= "") +
  scale_color_brewer(palette = "Set1")
```

Interestingly, we find a slightly negative effect, relative to self-reported extensive use, of using LLMs less (@fig-model-results). As expected, self-reported GPA is positively associated with final mark.


# Discussion {#sec-discussion}

There is some very mild evidence from this observational study that LLM usage may be associated with better scores. However, this work should mostly be treated as a direction for future research in this area. More broadly, our work has three major take-aways: 1) LLMs have a major role to play in enhancing the trustworthiness of data science. 2) Making general purpose chatbots more opinionated and knowledgeable about data science would considerably help (i.e. fine-tuning them - maybe talk more about the GPT store and the "Telling Stories with Data chatbot") 3) Alternative interaction approaches, beyond a chatbot interface, could be especially useful.

## Impact of LLMs on trustworthy data science

## General purpose compared with opinionated specific chatbots

## On the need for LLM-based infrastructure

## Weaknesses and next steps

There are a number of substantial weaknesses of this study. The foundational one is that we used observational data, much of which was self-reported. There may be selection bias present in terms of who used LLMs, who reported their LLM usage truthfully, and even who remained in the class. A different design, specifically a randomized controlled trial (RCT) would deal with many of these issues, although likely at some cost.

Regression reports average estimates over the full dataset. However, many earlier studies found distributional effects, with low-performing individuals benefiting more than high-performing individuals. Again, a change in design toward an RCT could enable the exploration of this question. Stratification of our dataset would result in small sample size, but nonetheless our data does provide some limited suggestive evidence that this effect may be present in data science. For instance, looking at the 28 students who received an A+ for the final paper, 13 of them had extensive LLM usage, whereas looking at the same number of worst performing students finds that only six of them had extensive LLM usage.

Along these lines, we have considered LLM usage for code and writing as equivalent, but they should actually have different impacts depending on student backgrounds. Distinguishing between native and second-language English speakers, and then focusing on differential LLM usage, could have added a great deal of nuance to the analysis.

Finally, we only considered one outcome measure, namely grade on the final paper. Each paper required a considerable amount of time for the student to produce. If an LLM was found to reduce the time taken to produce a paper, without any reduction in quality, then that would be a similarly useful outcome.

Despite these shortcomings, our work clearly identifies a need for further research examining how LLMs can be used to develop a more trustworthy data science.


\newpage

\appendix

# Appendix {-}

# Survey questions {#sec-survey-details}

1. After carefully reading the informed consent document, please indicate below whether you consent to have your anonymized responses included in the research study?
    - Yes, I authorize the use of the data collected about me for the STA302 course survey to be used. I will be compensated 1% of my course grade for completing the survey.
    - No I do not want my data included in the research study, but I want to complete the survey. I will be compensated 1% of my course grade for completing the survey.
    - I do not want to complete this survey. I realize that I am forfeiting the corresponding course credit.
2. What is your full name on Quercus?
3. What is your Student ID?
4. What year are you?
5. What is your specialization?
6. What is/are your major/s?
7. What is/are your minor/s?
8. What is your GPA?
9. Please rate how much each statement describes you, on a scale from "This is very different to me" to "This is a lot like me" ["This is very different to me"; "This somewhat describes me"; "This is a lot like me"]
    - Writing is easy for me
    - I like to write
    - I believe it is important to be a good writer.
    - When I edit it is easy for me to catch my mistakes.
    - I feel confident sharing my writing.
    - I am confident in my overall writing ability.
10. Please rate how much each statement describes you, on a scale from "This is very different to me" to "This is a lot like me". When answering, please consider whichever programming language you are most familiar with. ["This is very different to me"; "This somewhat describes me"; "This is a lot like me"]
    - Coding is easy for me
    - I like to code
    - I believe it is important to be a good coder.
    - When I check my code it is easy for me to catch my mistakes.
    - I feel confident sharing my code.
    - I am confident in my overall coding ability.
11. How familiar are you with using generative AI tools such as OpenAI's ChatGPT or equivalents?
    - Very familiar
    - Somewhat familiar
    - Not familiar
    - Other
12. Have you used any generative AI tools such as OpenAI's ChatGPT or equivalents for any reason (personal or educational)?
    - Yes
    - No
    - Other
13. If you have used generative AI tools such as OpenAI's ChatGPT or equivalents, in what ways have you used it (select all that apply)?
    - Asking technical questions
    - Carrying on a conversation out of curiosity
    - Asking general knowledge questions
    - Solving homework
    - Checking solutions
    - Asking quick questions when stuck
    - Explaining concepts
    - Writing essays or paragraphs
    - Writing code
    - Never used it
    - Other
14. To what extent do you think using generative AI tools such as ChatGPT by OpenAI (or equivalents) is ethical and appropriate for schoolwork?
    - Appropriate
    - Inappropriate
    - Other
15. Please elaborate on your answer above.
16. Did you use any generative AI tools such as OpenAI's ChatGPT or equivalents for STA302?
    - Yes
    - No
    - Other
17. How helpful did you find generative AI tools such as ChatGPT by OpenAI (or equivalents) for each component of STA302? ["Not helpful"; "Somewhat helpful"; "Very helpful"; "I did not use generative AI for this component"]
    - Weekly quiz
    - Weekly mini-essay
    - Papers: Generating ideas
    - Papers: Writing code
    - Papers: Writing content
    - Papers: Improving content
18. What is your recommendation for how generative AI tools such as ChatGPT by OpenAI (or equivalents) should be used in the course in future?
19. (Optional) Any other comments?

\newpage

# Model details {#sec-model-details}

We use `bayesplot` [@bayesplot] and `loo` [@loo] help with posterior predictive checks and model diagnostics.

## Posterior predictive check

```{r fig-posteriorchecks}
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Posterior predictive checking
#| fig-subcap: ["Base model", "Including self-reported GPA"]
#| layout-ncol: 2

bayesplot::pp_check(fit1) +
  theme_classic() +
  theme(legend.position = "bottom")

bayesplot::pp_check(fit2) +
  theme_classic() +
  theme(legend.position = "bottom")
```


```{r tbl-loo}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Model comparison

brms::loo_compare(loo::loo(fit1, cores = 2), loo::loo(fit2, cores = 2))
```  

\newpage

## Diagnostics

```{r fig-basemodeldiagnostics}
#| fig-height: 7
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Base model diagnostics 

plot(fit1)
```

```{r fig-gpamodeldiagnostics}
#| fig-height: 7
#| message: false
#| echo: false
#| warning: false
#| fig-cap: Model including self-reported GPA diagnostics 

plot(fit2)
```

\newpage

# References
